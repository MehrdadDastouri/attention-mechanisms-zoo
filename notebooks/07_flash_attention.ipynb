{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Flash Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Re, C. (2022).** *FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.* NeurIPS. [arXiv:2205.14135](https://arxiv.org/abs/2205.14135)\n",
                "\n",
                "---\n",
                "\n",
                "## Key Insight\n",
                "\n",
                "Standard attention is **memory-bound**, not compute-bound. The bottleneck is reading/writing the $O(n^2)$ attention matrix to GPU HBM (High Bandwidth Memory).\n",
                "\n",
                "### Memory Hierarchy\n",
                "\n",
                "- **SRAM** (on-chip): ~20MB, ~19TB/s bandwidth\n",
                "- **HBM** (GPU memory): ~40GB, ~1.5TB/s bandwidth\n",
                "\n",
                "### FlashAttention Strategy\n",
                "\n",
                "1. **Tiling**: Process attention in blocks that fit in SRAM\n",
                "2. **Recomputation**: Don't store $O(n^2)$ matrix, recompute in backward\n",
                "3. **Online Softmax**: Compute softmax incrementally across blocks\n",
                "\n",
                "### Algorithm\n",
                "\n",
                "```\n",
                "For each block of Q:\n",
                "    Load Q_block to SRAM\n",
                "    Initialize running max m, sum l, output O\n",
                "    For each block of K, V:\n",
                "        Load K_block, V_block to SRAM\n",
                "        Compute S_block = Q_block @ K_block^T\n",
                "        Update m, l, O using online softmax\n",
                "    Write O to HBM\n",
                "```\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- Time: $O(n^2 d)$ (same as standard)\n",
                "- Space: $O(n)$ instead of $O(n^2)$\n",
                "- IO: Reduced by factor of $O(n / \\text{block\\_size})$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def online_softmax_attention_block(q_block, k, v, block_size_kv=64):\n",
                "    \"\"\"Demonstrate online softmax for FlashAttention.\"\"\"\n",
                "    seq_len_k = k.size(1)\n",
                "    d_k = q_block.size(-1)\n",
                "    block_size_q = q_block.size(1)\n",
                "    \n",
                "    # Initialize accumulators\n",
                "    output = torch.zeros_like(q_block)\n",
                "    max_scores = torch.full((q_block.size(0), block_size_q, 1), float('-inf'))\n",
                "    sum_exp = torch.zeros((q_block.size(0), block_size_q, 1))\n",
                "    \n",
                "    # Process K, V in blocks\n",
                "    for kv_start in range(0, seq_len_k, block_size_kv):\n",
                "        kv_end = min(kv_start + block_size_kv, seq_len_k)\n",
                "        k_block = k[:, kv_start:kv_end]\n",
                "        v_block = v[:, kv_start:kv_end]\n",
                "        \n",
                "        # Compute scores for this block\n",
                "        scores = torch.matmul(q_block, k_block.transpose(-2, -1)) / (d_k ** 0.5)\n",
                "        \n",
                "        # Online softmax update\n",
                "        block_max = scores.max(dim=-1, keepdim=True).values\n",
                "        new_max = torch.maximum(max_scores, block_max)\n",
                "        \n",
                "        # Rescale previous values\n",
                "        exp_diff = torch.exp(max_scores - new_max)\n",
                "        sum_exp = sum_exp * exp_diff\n",
                "        output = output * exp_diff\n",
                "        \n",
                "        # Add new block contribution\n",
                "        exp_scores = torch.exp(scores - new_max)\n",
                "        sum_exp = sum_exp + exp_scores.sum(dim=-1, keepdim=True)\n",
                "        output = output + torch.matmul(exp_scores, v_block)\n",
                "        \n",
                "        max_scores = new_max\n",
                "    \n",
                "    return output / sum_exp.clamp(min=1e-6)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify correctness\n",
                "torch.manual_seed(42)\n",
                "batch, seq_len, d = 2, 64, 32\n",
                "\n",
                "q = torch.randn(batch, seq_len, d)\n",
                "k = torch.randn(batch, seq_len, d)\n",
                "v = torch.randn(batch, seq_len, d)\n",
                "\n",
                "# Standard attention\n",
                "scores = torch.matmul(q, k.transpose(-2, -1)) / (d ** 0.5)\n",
                "weights = F.softmax(scores, dim=-1)\n",
                "standard_output = torch.matmul(weights, v)\n",
                "\n",
                "# Online softmax (Flash-style)\n",
                "flash_output = online_softmax_attention_block(q, k, v, block_size_kv=16)\n",
                "\n",
                "print(f\"Max absolute difference: {(standard_output - flash_output).abs().max():.6f}\")\n",
                "print(f\"Outputs match: {torch.allclose(standard_output, flash_output, atol=1e-5)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch 2.0+ native flash attention\n",
                "if hasattr(F, 'scaled_dot_product_attention'):\n",
                "    print(\"PyTorch SDPA available (includes Flash Attention backend)\")\n",
                "    \n",
                "    # This automatically uses Flash Attention when possible\n",
                "    q_t = q.unsqueeze(1)  # Add head dimension\n",
                "    k_t = k.unsqueeze(1)\n",
                "    v_t = v.unsqueeze(1)\n",
                "    \n",
                "    output = F.scaled_dot_product_attention(q_t, k_t, v_t)\n",
                "    print(f\"SDPA output shape: {output.squeeze(1).shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Memory Savings\n",
                "\n",
                "| Method | Attention Memory | Total Memory |\n",
                "|--------|-----------------|-------------|\n",
                "| Standard | $O(n^2)$ | $O(n^2 + nd)$ |\n",
                "| FlashAttention | $O(1)$ per block | $O(n)$ |\n",
                "\n",
                "For sequence length 2048 with float16:\n",
                "- Standard: 2048 x 2048 x 2 bytes = 8 MB per head\n",
                "- Flash: Does not materialize full matrix"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}