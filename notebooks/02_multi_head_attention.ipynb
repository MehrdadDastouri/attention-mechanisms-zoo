{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Head Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Vaswani, A., et al. (2017).** *Attention Is All You Need.* NeurIPS. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
                "\n",
                "---\n",
                "\n",
                "## Mathematical Derivation\n",
                "\n",
                "### Core Formula\n",
                "\n",
                "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
                "\n",
                "Where each head is:\n",
                "\n",
                "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
                "\n",
                "### Projection Matrices\n",
                "\n",
                "- $W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
                "- $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
                "- $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$\n",
                "- $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$\n",
                "\n",
                "Typically: $d_k = d_v = d_{model} / h$\n",
                "\n",
                "### Why Multiple Heads?\n",
                "\n",
                "1. **Subspace Learning**: Each head can learn different attention patterns\n",
                "2. **Position Focus**: Different heads attend to different positions\n",
                "3. **Ensemble Effect**: Aggregates information from multiple representations\n",
                "\n",
                "### Complexity Analysis\n",
                "\n",
                "| Operation | Time Complexity | Space Complexity |\n",
                "|-----------|-----------------|------------------|\n",
                "| Projections | $O(n \\cdot d^2)$ | $O(d^2)$ |\n",
                "| Attention per head | $O(n^2 \\cdot d_k)$ | $O(n^2)$ |\n",
                "| All heads | $O(n^2 \\cdot d)$ | $O(n^2 \\cdot h)$ |\n",
                "| Output projection | $O(n \\cdot d^2)$ | $O(d^2)$ |\n",
                "| **Total** | $O(n^2 \\cdot d)$ | $O(n^2 \\cdot h)$ |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "from typing import Optional, Tuple\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n",
                "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.0) -> None:\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.d_model = d_model\n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
                "        \n",
                "        self.w_q = nn.Linear(d_model, d_model)\n",
                "        self.w_k = nn.Linear(d_model, d_model)\n",
                "        self.w_v = nn.Linear(d_model, d_model)\n",
                "        self.w_o = nn.Linear(d_model, d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(\n",
                "        self,\n",
                "        query: torch.Tensor,\n",
                "        key: torch.Tensor,\n",
                "        value: torch.Tensor,\n",
                "        mask: Optional[torch.Tensor] = None\n",
                "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        batch_size, seq_len_q = query.size(0), query.size(1)\n",
                "        seq_len_k = key.size(1)\n",
                "        \n",
                "        # Project and reshape: (batch, heads, seq, d_k)\n",
                "        q = self.w_q(query).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        k = self.w_k(key).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        v = self.w_v(value).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        # Attention scores: (batch, heads, seq_q, seq_k)\n",
                "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
                "        \n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask.bool(), float(\"-inf\"))\n",
                "        \n",
                "        attn_weights = F.softmax(scores, dim=-1)\n",
                "        attn_weights = self.dropout(attn_weights)\n",
                "        \n",
                "        # Attended values\n",
                "        attended = torch.matmul(attn_weights, v)\n",
                "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
                "        \n",
                "        return self.w_o(attended), attn_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstration\n",
                "torch.manual_seed(42)\n",
                "\n",
                "batch_size, seq_len, d_model, num_heads = 1, 8, 64, 4\n",
                "attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
                "\n",
                "x = torch.randn(batch_size, seq_len, d_model)\n",
                "output, weights = attention(x, x, x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {weights.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention patterns for each head\n",
                "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
                "\n",
                "for head_idx in range(num_heads):\n",
                "    sns.heatmap(\n",
                "        weights[0, head_idx].detach().numpy(),\n",
                "        ax=axes[head_idx],\n",
                "        cmap='viridis',\n",
                "        square=True,\n",
                "        cbar=head_idx == num_heads - 1\n",
                "    )\n",
                "    axes[head_idx].set_title(f'Head {head_idx}')\n",
                "    axes[head_idx].set_xlabel('Key')\n",
                "    if head_idx == 0:\n",
                "        axes[head_idx].set_ylabel('Query')\n",
                "\n",
                "plt.suptitle('Multi-Head Attention Patterns')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## When to Use Multi-Head Attention\n",
                "\n",
                "| Use Case | Recommendation |\n",
                "|----------|---------------|\n",
                "| Standard transformer | 8-16 heads |\n",
                "| Small models | 4-8 heads |\n",
                "| Large models | 16-64 heads |\n",
                "| Memory constrained | Consider MQA/GQA |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}