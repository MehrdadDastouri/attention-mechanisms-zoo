{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sparse Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Child, R., Gray, S., Radford, A., & Sutskever, I. (2019).** *Generating Long Sequences with Sparse Transformers.* [arXiv:1904.10509](https://arxiv.org/abs/1904.10509)\n",
                "\n",
                "---\n",
                "\n",
                "## Mathematical Derivation\n",
                "\n",
                "### Motivation\n",
                "\n",
                "Standard attention has $O(n^2)$ complexity. For long sequences, this becomes prohibitive. Sparse attention reduces this by attending to only a subset of positions.\n",
                "\n",
                "### Sparsity Patterns\n",
                "\n",
                "**1. Strided Pattern**: Position $i$ attends to positions $\\{i-k, i-2k, ..., 0\\}$ plus local context.\n",
                "\n",
                "**2. Fixed Pattern**: Position $i$ attends to a fixed block containing $i$.\n",
                "\n",
                "**3. Combined**: Alternating or combined patterns.\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- Time: $O(n \\cdot \\sqrt{n} \\cdot d)$ with block size $\\sqrt{n}$\n",
                "- Space: $O(n \\cdot \\sqrt{n})$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sparse_mask(seq_len: int, block_size: int, pattern: str = 'combined'):\n",
                "    \"\"\"Create sparse attention mask.\"\"\"\n",
                "    mask = torch.ones(seq_len, seq_len, dtype=torch.bool)\n",
                "    \n",
                "    if pattern in ('fixed', 'combined'):\n",
                "        # Local block attention\n",
                "        for i in range(seq_len):\n",
                "            block_start = (i // block_size) * block_size\n",
                "            block_end = min(block_start + block_size, seq_len)\n",
                "            mask[i, block_start:block_end] = False\n",
                "    \n",
                "    if pattern in ('strided', 'combined'):\n",
                "        # Strided attention\n",
                "        for i in range(seq_len):\n",
                "            for j in range(0, seq_len, block_size):\n",
                "                mask[i, j] = False\n",
                "    \n",
                "    return mask"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize different sparsity patterns\n",
                "seq_len, block_size = 16, 4\n",
                "patterns = ['fixed', 'strided', 'combined']\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "for idx, pattern in enumerate(patterns):\n",
                "    mask = create_sparse_mask(seq_len, block_size, pattern)\n",
                "    attention_pattern = (~mask).float().numpy()\n",
                "    \n",
                "    sns.heatmap(attention_pattern, ax=axes[idx], cmap='Blues', square=True, cbar=False)\n",
                "    axes[idx].set_title(f'{pattern.capitalize()} Pattern')\n",
                "    axes[idx].set_xlabel('Key Position')\n",
                "    axes[idx].set_ylabel('Query Position')\n",
                "\n",
                "plt.suptitle(f'Sparse Attention Patterns (block_size={block_size})')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count attended positions\n",
                "for pattern in patterns:\n",
                "    mask = create_sparse_mask(seq_len, block_size, pattern)\n",
                "    attended = (~mask).sum().item()\n",
                "    full = seq_len * seq_len\n",
                "    print(f\"{pattern}: {attended}/{full} positions ({100*attended/full:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## When to Use Sparse Attention\n",
                "\n",
                "| Scenario | Sparse Attention? |\n",
                "|----------|------------------|\n",
                "| Sequence > 1024 | Consider it |\n",
                "| Sequence > 4096 | Recommended |\n",
                "| Need global context | Use global tokens |\n",
                "| Local patterns sufficient | Use fixed pattern |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}