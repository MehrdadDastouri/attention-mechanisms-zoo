{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Query Attention (MQA)\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Shazeer, N. (2019).** *Fast Transformer Decoding: One Write-Head is All You Need.* [arXiv:1911.02150](https://arxiv.org/abs/1911.02150)\n",
                "\n",
                "---\n",
                "\n",
                "## Key Insight\n",
                "\n",
                "In standard MHA, each head has its own Q, K, V projections. MQA uses:\n",
                "- **Multiple query heads** (like MHA)\n",
                "- **Single key head** (shared across all queries)\n",
                "- **Single value head** (shared across all queries)\n",
                "\n",
                "### Memory Bandwidth Savings\n",
                "\n",
                "During inference with KV caching:\n",
                "- **MHA**: Load K, V for each head: $O(n \\cdot d_k \\cdot h)$\n",
                "- **MQA**: Load K, V once: $O(n \\cdot d_k)$\n",
                "\n",
                "Savings: $h$ times less KV cache memory.\n",
                "\n",
                "### Used In\n",
                "\n",
                "- PaLM (Google)\n",
                "- Falcon (TII)\n",
                "- StarCoder\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- Time: $O(n^2 d)$ (same as MHA)\n",
                "- KV Cache: $O(n \\cdot d_k)$ vs $O(n \\cdot d_k \\cdot h)$ for MHA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiQueryAttention(nn.Module):\n",
                "    \"\"\"Multi-Query Attention: single K/V head, multiple Q heads.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model: int, num_heads: int = 8) -> None:\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        self.d_model = d_model\n",
                "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
                "        \n",
                "        # Q: full d_model projection for all heads\n",
                "        self.w_q = nn.Linear(d_model, d_model)\n",
                "        # K, V: single head projection\n",
                "        self.w_k = nn.Linear(d_model, self.d_k)\n",
                "        self.w_v = nn.Linear(d_model, self.d_k)\n",
                "        self.w_o = nn.Linear(d_model, d_model)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor):\n",
                "        batch, seq_len = x.size(0), x.size(1)\n",
                "        \n",
                "        # Multiple query heads\n",
                "        q = self.w_q(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        # Single K, V\n",
                "        k = self.w_k(x)  # (batch, seq, d_k)\n",
                "        v = self.w_v(x)  # (batch, seq, d_k)\n",
                "        \n",
                "        # Broadcast K, V across heads\n",
                "        scores = torch.matmul(q, k.unsqueeze(1).transpose(-2, -1)) * self.scale\n",
                "        attn = F.softmax(scores, dim=-1)\n",
                "        \n",
                "        attended = torch.matmul(attn, v.unsqueeze(1))\n",
                "        attended = attended.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)\n",
                "        \n",
                "        return self.w_o(attended), attn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare parameter counts\n",
                "d_model, num_heads = 512, 8\n",
                "\n",
                "# MHA parameters for K, V projections: 2 * d_model * d_model\n",
                "mha_kv_params = 2 * d_model * d_model\n",
                "\n",
                "# MQA parameters for K, V projections: 2 * d_model * d_k\n",
                "d_k = d_model // num_heads\n",
                "mqa_kv_params = 2 * d_model * d_k\n",
                "\n",
                "print(f\"MHA K/V projection parameters: {mha_kv_params:,}\")\n",
                "print(f\"MQA K/V projection parameters: {mqa_kv_params:,}\")\n",
                "print(f\"Reduction factor: {mha_kv_params / mqa_kv_params:.1f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# KV cache size comparison\n",
                "seq_len = 2048\n",
                "batch_size = 32\n",
                "\n",
                "mha_kv_cache = batch_size * seq_len * d_model * 2  # K and V\n",
                "mqa_kv_cache = batch_size * seq_len * d_k * 2\n",
                "\n",
                "print(f\"MHA KV cache per layer: {mha_kv_cache * 2 / 1024**2:.2f} MB (fp16)\")\n",
                "print(f\"MQA KV cache per layer: {mqa_kv_cache * 2 / 1024**2:.2f} MB (fp16)\")\n",
                "print(f\"Memory savings: {num_heads}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Trade-offs\n",
                "\n",
                "| Aspect | MHA | MQA |\n",
                "|--------|-----|-----|\n",
                "| Quality | Higher | Slightly lower |\n",
                "| KV Cache | $h \\times$ larger | $1 \\times$ |\n",
                "| Inference Speed | Baseline | Faster |\n",
                "| Use Case | Training | Inference-optimized |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}