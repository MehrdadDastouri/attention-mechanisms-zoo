{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Linear Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020).** *Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention.* ICML. [arXiv:2006.16236](https://arxiv.org/abs/2006.16236)\n",
                "\n",
                "---\n",
                "\n",
                "## Mathematical Derivation\n",
                "\n",
                "### Key Insight\n",
                "\n",
                "Standard attention: $\\text{softmax}(QK^T)V$ requires materializing the $n \\times n$ matrix.\n",
                "\n",
                "Linear attention uses a kernel trick. If we can write:\n",
                "\n",
                "$$\\text{sim}(q, k) = \\phi(q)^T \\phi(k)$$\n",
                "\n",
                "Then:\n",
                "\n",
                "$$\\text{Attention}_i = \\frac{\\sum_j \\phi(q_i)^T \\phi(k_j) v_j}{\\sum_j \\phi(q_i)^T \\phi(k_j)}$$\n",
                "\n",
                "$$= \\frac{\\phi(q_i)^T \\sum_j \\phi(k_j) v_j^T}{\\phi(q_i)^T \\sum_j \\phi(k_j)}$$\n",
                "\n",
                "### Order of Operations\n",
                "\n",
                "Standard: $(QK^T)V \\rightarrow O(n^2 d)$\n",
                "\n",
                "Linear: $Q(K^T V) \\rightarrow O(n d^2)$\n",
                "\n",
                "### Feature Maps\n",
                "\n",
                "- **ELU+1**: $\\phi(x) = \\text{ELU}(x) + 1$ (ensures positivity)\n",
                "- **ReLU**: $\\phi(x) = \\text{ReLU}(x)$\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- Time: $O(n \\cdot d^2)$\n",
                "- Space: $O(n \\cdot d + d^2)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def elu_feature_map(x: torch.Tensor) -> torch.Tensor:\n",
                "    \"\"\"ELU + 1 feature map for linear attention.\"\"\"\n",
                "    return F.elu(x) + 1\n",
                "\n",
                "def linear_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, eps: float = 1e-6):\n",
                "    \"\"\"Compute linear attention with O(n*d^2) complexity.\"\"\"\n",
                "    # Apply feature map\n",
                "    q = elu_feature_map(q)\n",
                "    k = elu_feature_map(k)\n",
                "    \n",
                "    # K^T @ V: (d_k, d_v) - aggregated key-value\n",
                "    kv = torch.einsum('bnd,bnv->bdv', k, v)\n",
                "    \n",
                "    # Q @ (K^T @ V): (batch, n, d_v)\n",
                "    numerator = torch.einsum('bnd,bdv->bnv', q, kv)\n",
                "    \n",
                "    # Normalization\n",
                "    k_sum = k.sum(dim=1)  # (batch, d_k)\n",
                "    denominator = torch.einsum('bnd,bd->bn', q, k_sum).unsqueeze(-1).clamp(min=eps)\n",
                "    \n",
                "    return numerator / denominator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare complexity\n",
                "import time\n",
                "\n",
                "def standard_attention(q, k, v):\n",
                "    scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
                "    weights = F.softmax(scores, dim=-1)\n",
                "    return torch.matmul(weights, v)\n",
                "\n",
                "seq_lengths = [128, 256, 512, 1024, 2048]\n",
                "d_model = 64\n",
                "batch_size = 4\n",
                "\n",
                "standard_times = []\n",
                "linear_times = []\n",
                "\n",
                "for seq_len in seq_lengths:\n",
                "    q = torch.randn(batch_size, seq_len, d_model)\n",
                "    k = torch.randn(batch_size, seq_len, d_model)\n",
                "    v = torch.randn(batch_size, seq_len, d_model)\n",
                "    \n",
                "    start = time.perf_counter()\n",
                "    for _ in range(10):\n",
                "        _ = standard_attention(q, k, v)\n",
                "    standard_times.append((time.perf_counter() - start) / 10 * 1000)\n",
                "    \n",
                "    start = time.perf_counter()\n",
                "    for _ in range(10):\n",
                "        _ = linear_attention(q, k, v)\n",
                "    linear_times.append((time.perf_counter() - start) / 10 * 1000)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(seq_lengths, standard_times, 'o-', label='Standard O(n^2 d)')\n",
                "plt.plot(seq_lengths, linear_times, 's-', label='Linear O(n d^2)')\n",
                "plt.xlabel('Sequence Length')\n",
                "plt.ylabel('Time (ms)')\n",
                "plt.title('Standard vs Linear Attention Scaling')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Trade-offs\n",
                "\n",
                "| Aspect | Standard | Linear |\n",
                "|--------|----------|--------|\n",
                "| Time | $O(n^2 d)$ | $O(n d^2)$ |\n",
                "| Better when | $n < d$ | $n > d$ |\n",
                "| Attention quality | Exact softmax | Approximate |\n",
                "| Causal support | Mask-based | RNN-style |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}