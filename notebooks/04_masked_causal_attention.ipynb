{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Masked Causal Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Vaswani, A., et al. (2017).** *Attention Is All You Need.* NeurIPS. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
                "\n",
                "---\n",
                "\n",
                "## Mathematical Derivation\n",
                "\n",
                "### Causal Masking\n",
                "\n",
                "For autoregressive generation, each position can only attend to previous positions:\n",
                "\n",
                "$$\\text{Mask}_{ij} = \\begin{cases} 0 & \\text{if } j \\leq i \\\\ -\\infty & \\text{if } j > i \\end{cases}$$\n",
                "\n",
                "$$\\text{CausalAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\text{Mask}\\right)V$$\n",
                "\n",
                "### Lower Triangular Mask\n",
                "\n",
                "```\n",
                "Position:  0  1  2  3\n",
                "Query 0:  [1  0  0  0]  <- can only see position 0\n",
                "Query 1:  [1  1  0  0]  <- can see positions 0, 1\n",
                "Query 2:  [1  1  1  0]  <- can see positions 0, 1, 2\n",
                "Query 3:  [1  1  1  1]  <- can see all positions\n",
                "```\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- Time: $O(n^2 \\cdot d)$ (same as standard, but half the actual work)\n",
                "- Space: $O(n^2)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CausalAttention(nn.Module):\n",
                "    \"\"\"Causal (Masked) Attention for autoregressive models.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model: int, num_heads: int = 8, max_seq_len: int = 2048) -> None:\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        self.d_model = d_model\n",
                "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
                "        \n",
                "        self.w_q = nn.Linear(d_model, d_model)\n",
                "        self.w_k = nn.Linear(d_model, d_model)\n",
                "        self.w_v = nn.Linear(d_model, d_model)\n",
                "        self.w_o = nn.Linear(d_model, d_model)\n",
                "        \n",
                "        # Precompute causal mask\n",
                "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
                "        self.register_buffer('causal_mask', mask)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor):\n",
                "        batch_size, seq_len = x.size(0), x.size(1)\n",
                "        \n",
                "        q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        k = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        v = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
                "        scores = scores.masked_fill(self.causal_mask[:seq_len, :seq_len], float(\"-inf\"))\n",
                "        \n",
                "        attn_weights = F.softmax(scores, dim=-1)\n",
                "        \n",
                "        attended = torch.matmul(attn_weights, v)\n",
                "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
                "        \n",
                "        return self.w_o(attended), attn_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate causal masking\n",
                "torch.manual_seed(42)\n",
                "\n",
                "batch_size, seq_len, d_model, num_heads = 1, 8, 64, 4\n",
                "causal_attn = CausalAttention(d_model=d_model, num_heads=num_heads)\n",
                "\n",
                "x = torch.randn(batch_size, seq_len, d_model)\n",
                "output, weights = causal_attn(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize causal attention pattern\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Show the causal mask\n",
                "causal_mask = causal_attn.causal_mask[:seq_len, :seq_len].float().numpy()\n",
                "sns.heatmap(1 - causal_mask, ax=axes[0], cmap='Blues', square=True, cbar=False,\n",
                "            annot=True, fmt='.0f')\n",
                "axes[0].set_title('Causal Mask (1=attend, 0=masked)')\n",
                "axes[0].set_xlabel('Key Position')\n",
                "axes[0].set_ylabel('Query Position')\n",
                "\n",
                "# Show actual attention weights (averaged across heads)\n",
                "avg_weights = weights[0].mean(dim=0).detach().numpy()\n",
                "sns.heatmap(avg_weights, ax=axes[1], cmap='viridis', square=True, annot=True, fmt='.2f')\n",
                "axes[1].set_title('Causal Attention Weights')\n",
                "axes[1].set_xlabel('Key Position')\n",
                "axes[1].set_ylabel('Query Position')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify upper triangle is zero\n",
                "for i in range(seq_len):\n",
                "    for j in range(i + 1, seq_len):\n",
                "        assert weights[0, :, i, j].sum() == 0, f\"Position ({i}, {j}) should be masked\"\n",
                "print(\"Verified: All future positions are properly masked.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Use Cases for Causal Attention\n",
                "\n",
                "| Model | Purpose |\n",
                "|-------|--------|\n",
                "| GPT | Language modeling |\n",
                "| Decoder-only LLMs | Text generation |\n",
                "| Autoregressive models | Any sequential prediction |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}