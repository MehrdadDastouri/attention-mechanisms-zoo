{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Cross-Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Vaswani, A., et al. (2017).** *Attention Is All You Need.* NeurIPS. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
                "\n",
                "---\n",
                "\n",
                "## Mathematical Derivation\n",
                "\n",
                "### Core Concept\n",
                "\n",
                "In cross-attention, queries come from one sequence (decoder) while keys and values come from another (encoder):\n",
                "\n",
                "$$\\text{CrossAttention}(Q_{dec}, K_{enc}, V_{enc}) = \\text{softmax}\\left(\\frac{Q_{dec}K_{enc}^T}{\\sqrt{d_k}}\\right)V_{enc}$$\n",
                "\n",
                "### Use Cases\n",
                "\n",
                "1. **Machine Translation**: Decoder attends to source sentence\n",
                "2. **Image Captioning**: Text decoder attends to image features\n",
                "3. **Speech Recognition**: Text decoder attends to audio features\n",
                "\n",
                "### Dimensions\n",
                "\n",
                "- Query: $(batch, n, d_{model})$ where $n$ = decoder sequence length\n",
                "- Key/Value: $(batch, m, d_{model})$ where $m$ = encoder sequence length\n",
                "- Output: $(batch, n, d_{model})$\n",
                "- Attention: $(batch, h, n, m)$\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- Time: $O(n \\cdot m \\cdot d)$\n",
                "- Space: $O(n \\cdot m)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "from typing import Optional, Tuple\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CrossAttention(nn.Module):\n",
                "    \"\"\"Cross-Attention for encoder-decoder architectures.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.0) -> None:\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
                "        \n",
                "        self.w_q = nn.Linear(d_model, d_model)  # From decoder\n",
                "        self.w_k = nn.Linear(d_model, d_model)  # From encoder\n",
                "        self.w_v = nn.Linear(d_model, d_model)  # From encoder\n",
                "        self.w_o = nn.Linear(d_model, d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.d_model = d_model\n",
                "    \n",
                "    def forward(\n",
                "        self,\n",
                "        decoder_hidden: torch.Tensor,\n",
                "        encoder_output: torch.Tensor,\n",
                "        mask: Optional[torch.Tensor] = None\n",
                "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        \"\"\"Compute cross-attention.\n",
                "        \n",
                "        Args:\n",
                "            decoder_hidden: Decoder states (batch, n, d_model)\n",
                "            encoder_output: Encoder outputs (batch, m, d_model)\n",
                "            mask: Optional encoder padding mask\n",
                "        \"\"\"\n",
                "        batch_size = decoder_hidden.size(0)\n",
                "        seq_len_q = decoder_hidden.size(1)\n",
                "        seq_len_k = encoder_output.size(1)\n",
                "        \n",
                "        q = self.w_q(decoder_hidden).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        k = self.w_k(encoder_output).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        v = self.w_v(encoder_output).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
                "        \n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask.bool(), float(\"-inf\"))\n",
                "        \n",
                "        attn_weights = F.softmax(scores, dim=-1)\n",
                "        attn_weights = self.dropout(attn_weights)\n",
                "        \n",
                "        attended = torch.matmul(attn_weights, v)\n",
                "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
                "        \n",
                "        return self.w_o(attended), attn_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate translation: source (encoder) and target (decoder) have different lengths\n",
                "torch.manual_seed(42)\n",
                "\n",
                "batch_size, d_model, num_heads = 1, 64, 4\n",
                "encoder_len, decoder_len = 10, 6  # Different lengths\n",
                "\n",
                "encoder_output = torch.randn(batch_size, encoder_len, d_model)\n",
                "decoder_hidden = torch.randn(batch_size, decoder_len, d_model)\n",
                "\n",
                "cross_attn = CrossAttention(d_model=d_model, num_heads=num_heads)\n",
                "output, weights = cross_attn(decoder_hidden, encoder_output)\n",
                "\n",
                "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
                "print(f\"Decoder hidden shape: {decoder_hidden.shape}\")\n",
                "print(f\"Cross-attention output: {output.shape}\")\n",
                "print(f\"Attention weights: {weights.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize cross-attention\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "# Average across heads\n",
                "avg_weights = weights[0].mean(dim=0).detach().numpy()\n",
                "\n",
                "sns.heatmap(\n",
                "    avg_weights,\n",
                "    ax=ax,\n",
                "    cmap='viridis',\n",
                "    annot=True,\n",
                "    fmt='.2f',\n",
                "    xticklabels=[f'Enc_{i}' for i in range(encoder_len)],\n",
                "    yticklabels=[f'Dec_{i}' for i in range(decoder_len)]\n",
                ")\n",
                "ax.set_xlabel('Encoder Position (Source)')\n",
                "ax.set_ylabel('Decoder Position (Target)')\n",
                "ax.set_title('Cross-Attention: How Decoder Attends to Encoder')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## When to Use Cross-Attention\n",
                "\n",
                "| Task | Cross-Attention Role |\n",
                "|------|---------------------|\n",
                "| Translation | Align target words to source |\n",
                "| Summarization | Focus on important source parts |\n",
                "| Image Captioning | Attend to image regions |\n",
                "| VQA | Query image with question |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}