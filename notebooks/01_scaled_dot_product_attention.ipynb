{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Scaled Dot-Product Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017).** *Attention Is All You Need.* Advances in Neural Information Processing Systems (NeurIPS). [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
                "\n",
                "---\n",
                "\n",
                "## Mathematical Derivation\n",
                "\n",
                "### Core Formula\n",
                "\n",
                "The scaled dot-product attention is defined as:\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
                "\n",
                "Where:\n",
                "- $Q \\in \\mathbb{R}^{n \\times d_k}$: Query matrix\n",
                "- $K \\in \\mathbb{R}^{m \\times d_k}$: Key matrix\n",
                "- $V \\in \\mathbb{R}^{m \\times d_v}$: Value matrix\n",
                "- $d_k$: Dimension of keys/queries\n",
                "- $n$: Number of queries\n",
                "- $m$: Number of keys/values\n",
                "\n",
                "### Step-by-Step Derivation\n",
                "\n",
                "**Step 1: Compute Attention Scores**\n",
                "\n",
                "$$S = QK^T \\in \\mathbb{R}^{n \\times m}$$\n",
                "\n",
                "Each element $S_{ij}$ represents the dot product similarity between query $i$ and key $j$:\n",
                "\n",
                "$$S_{ij} = q_i \\cdot k_j = \\sum_{l=1}^{d_k} q_{il} \\cdot k_{jl}$$\n",
                "\n",
                "**Step 2: Apply Scaling**\n",
                "\n",
                "$$\\tilde{S} = \\frac{S}{\\sqrt{d_k}}$$\n",
                "\n",
                "The scaling factor $\\sqrt{d_k}$ is crucial. Without it, for large $d_k$, the dot products grow large in magnitude, pushing the softmax into regions with extremely small gradients.\n",
                "\n",
                "**Why scaling?** Assuming $q$ and $k$ are independent random variables with mean 0 and variance 1:\n",
                "- $\\mathbb{E}[q \\cdot k] = 0$\n",
                "- $\\text{Var}[q \\cdot k] = d_k$\n",
                "\n",
                "By dividing by $\\sqrt{d_k}$, we normalize the variance to 1.\n",
                "\n",
                "**Step 3: Apply Softmax**\n",
                "\n",
                "$$A = \\text{softmax}(\\tilde{S})$$\n",
                "\n",
                "Where softmax is applied row-wise:\n",
                "\n",
                "$$A_{ij} = \\frac{\\exp(\\tilde{S}_{ij})}{\\sum_{l=1}^{m} \\exp(\\tilde{S}_{il})}$$\n",
                "\n",
                "**Step 4: Weighted Sum of Values**\n",
                "\n",
                "$$\\text{Output} = AV \\in \\mathbb{R}^{n \\times d_v}$$\n",
                "\n",
                "Each output row is a weighted combination of value vectors:\n",
                "\n",
                "$$\\text{Output}_i = \\sum_{j=1}^{m} A_{ij} \\cdot v_j$$\n",
                "\n",
                "### Gradient Derivation\n",
                "\n",
                "Let $L$ be the loss function. We derive gradients for backpropagation.\n",
                "\n",
                "**Gradient w.r.t. V:**\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial V} = A^T \\frac{\\partial L}{\\partial \\text{Output}}$$\n",
                "\n",
                "**Gradient w.r.t. A:**\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial \\text{Output}} V^T$$\n",
                "\n",
                "**Gradient through softmax:**\n",
                "\n",
                "For each row $i$, let $a = A_i$ (attention weights for query $i$):\n",
                "\n",
                "$$\\frac{\\partial a_j}{\\partial \\tilde{s}_k} = a_j(\\delta_{jk} - a_k)$$\n",
                "\n",
                "Where $\\delta_{jk}$ is the Kronecker delta.\n",
                "\n",
                "**Gradient w.r.t. Q and K:**\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial Q} = \\frac{\\partial L}{\\partial \\tilde{S}} \\cdot \\frac{K}{\\sqrt{d_k}}$$\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial K} = \\frac{\\partial L}{\\partial \\tilde{S}}^T \\cdot \\frac{Q}{\\sqrt{d_k}}$$\n",
                "\n",
                "---\n",
                "\n",
                "## Complexity Analysis\n",
                "\n",
                "| Operation | Time Complexity | Space Complexity |\n",
                "|-----------|-----------------|------------------|\n",
                "| $QK^T$ | $O(n \\cdot m \\cdot d_k)$ | $O(n \\cdot m)$ |\n",
                "| Softmax | $O(n \\cdot m)$ | $O(n \\cdot m)$ |\n",
                "| $AV$ | $O(n \\cdot m \\cdot d_v)$ | $O(n \\cdot d_v)$ |\n",
                "| **Total** | $O(n \\cdot m \\cdot d)$ | $O(n \\cdot m)$ |\n",
                "\n",
                "For self-attention where $n = m$:\n",
                "- **Time**: $O(n^2 \\cdot d)$\n",
                "- **Space**: $O(n^2)$ for storing attention weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "from typing import Optional, Tuple\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('viridis')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ScaledDotProductAttention(nn.Module):\n",
                "    \"\"\"Scaled Dot-Product Attention mechanism.\n",
                "    \n",
                "    Implements: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
                "    \n",
                "    Attributes:\n",
                "        d_model: Model dimension (used for scaling).\n",
                "        dropout: Dropout layer for regularization.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, d_model: int, dropout: float = 0.0) -> None:\n",
                "        \"\"\"Initialize scaled dot-product attention.\n",
                "        \n",
                "        Args:\n",
                "            d_model: The dimensionality of the model.\n",
                "            dropout: Dropout probability.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.scale = 1.0 / math.sqrt(d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(\n",
                "        self,\n",
                "        query: torch.Tensor,\n",
                "        key: torch.Tensor,\n",
                "        value: torch.Tensor,\n",
                "        mask: Optional[torch.Tensor] = None\n",
                "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        \"\"\"Compute scaled dot-product attention.\n",
                "        \n",
                "        Args:\n",
                "            query: Query tensor (batch, seq_len_q, d_model).\n",
                "            key: Key tensor (batch, seq_len_k, d_model).\n",
                "            value: Value tensor (batch, seq_len_k, d_model).\n",
                "            mask: Optional mask where True indicates masked positions.\n",
                "        \n",
                "        Returns:\n",
                "            output: Attended values (batch, seq_len_q, d_model).\n",
                "            attention_weights: Attention distribution (batch, seq_len_q, seq_len_k).\n",
                "        \"\"\"\n",
                "        # Step 1 & 2: Compute scaled attention scores\n",
                "        # scores = Q @ K^T / sqrt(d_k)\n",
                "        scores = torch.matmul(query, key.transpose(-2, -1)) * self.scale\n",
                "        \n",
                "        # Apply mask if provided\n",
                "        if mask is not None:\n",
                "            scores = scores.masked_fill(mask.bool(), float(\"-inf\"))\n",
                "        \n",
                "        # Step 3: Apply softmax\n",
                "        attention_weights = F.softmax(scores, dim=-1)\n",
                "        attention_weights = torch.nan_to_num(attention_weights, nan=0.0)\n",
                "        \n",
                "        # Apply dropout\n",
                "        attention_weights = self.dropout(attention_weights)\n",
                "        \n",
                "        # Step 4: Weighted sum of values\n",
                "        output = torch.matmul(attention_weights, value)\n",
                "        \n",
                "        return output, attention_weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Demonstration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set random seed for reproducibility\n",
                "torch.manual_seed(42)\n",
                "\n",
                "# Create sample data\n",
                "batch_size = 1\n",
                "seq_len = 8\n",
                "d_model = 64\n",
                "\n",
                "# Initialize attention module\n",
                "attention = ScaledDotProductAttention(d_model=d_model)\n",
                "\n",
                "# Create sample Q, K, V tensors\n",
                "query = torch.randn(batch_size, seq_len, d_model)\n",
                "key = torch.randn(batch_size, seq_len, d_model)\n",
                "value = torch.randn(batch_size, seq_len, d_model)\n",
                "\n",
                "# Forward pass\n",
                "output, attention_weights = attention(query, key, value)\n",
                "\n",
                "print(f\"Query shape: {query.shape}\")\n",
                "print(f\"Key shape: {key.shape}\")\n",
                "print(f\"Value shape: {value.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {attention_weights.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention weights\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "\n",
                "weights = attention_weights[0].detach().numpy()\n",
                "\n",
                "sns.heatmap(\n",
                "    weights,\n",
                "    ax=ax,\n",
                "    cmap='viridis',\n",
                "    annot=True,\n",
                "    fmt='.2f',\n",
                "    square=True,\n",
                "    cbar_kws={'label': 'Attention Weight'}\n",
                ")\n",
                "\n",
                "ax.set_xlabel('Key Position')\n",
                "ax.set_ylabel('Query Position')\n",
                "ax.set_title('Scaled Dot-Product Attention Weights')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify attention weights sum to 1\n",
                "weight_sums = attention_weights.sum(dim=-1)\n",
                "print(f\"Attention weight row sums (should be 1.0):\\n{weight_sums[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Effect of Scaling Factor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate importance of scaling\n",
                "d_k_values = [1, 16, 64, 256, 512]\n",
                "fig, axes = plt.subplots(1, len(d_k_values), figsize=(16, 3))\n",
                "\n",
                "for idx, d_k in enumerate(d_k_values):\n",
                "    q = torch.randn(1, 4, d_k)\n",
                "    k = torch.randn(1, 4, d_k)\n",
                "    \n",
                "    # Scores without scaling\n",
                "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
                "    weights = F.softmax(scores, dim=-1)\n",
                "    \n",
                "    sns.heatmap(\n",
                "        weights[0].detach().numpy(),\n",
                "        ax=axes[idx],\n",
                "        cmap='viridis',\n",
                "        vmin=0, vmax=1,\n",
                "        square=True,\n",
                "        cbar=False\n",
                "    )\n",
                "    axes[idx].set_title(f'd_k = {d_k}')\n",
                "    axes[idx].set_xlabel('Key')\n",
                "    if idx == 0:\n",
                "        axes[idx].set_ylabel('Query')\n",
                "\n",
                "plt.suptitle('Attention Weights with Proper Scaling (1/sqrt(d_k))')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison: When to Use Scaled Dot-Product Attention\n",
                "\n",
                "| Aspect | Scaled Dot-Product | Alternative |\n",
                "|--------|-------------------|-------------|\n",
                "| **Simplicity** | Single-head, straightforward | Multi-head for more capacity |\n",
                "| **Efficiency** | Baseline O(n^2) | Linear attention for long sequences |\n",
                "| **Use Case** | Building block for other attention | Direct use in simple models |\n",
                "| **Memory** | Full attention matrix | Flash attention for memory efficiency |\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **Foundation**: Scaled dot-product attention is the fundamental building block for all transformer attention mechanisms.\n",
                "\n",
                "2. **Scaling is Critical**: Without the $1/\\sqrt{d_k}$ scaling, gradients become vanishingly small for large $d_k$.\n",
                "\n",
                "3. **Quadratic Complexity**: The $O(n^2)$ space complexity for storing attention weights becomes prohibitive for very long sequences."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}