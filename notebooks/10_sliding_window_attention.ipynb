{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sliding Window Attention\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Beltagy, I., Peters, M. E., & Cohan, A. (2020).** *Longformer: The Long-Document Transformer.* [arXiv:2004.05150](https://arxiv.org/abs/2004.05150)\n",
                "\n",
                "---\n",
                "\n",
                "## Key Insight\n",
                "\n",
                "Not all tasks require full global attention. For many NLP tasks, local context is sufficient. Sliding window attention restricts each position to attend only within a fixed window.\n",
                "\n",
                "### Formula\n",
                "\n",
                "For position $i$ with window size $w$:\n",
                "\n",
                "$$\\text{Attend}(i) = \\{j : |i - j| \\leq w\\}$$\n",
                "\n",
                "### Effective Receptive Field\n",
                "\n",
                "With $L$ layers and window size $w$, the receptive field grows to $L \\times w$ through stacking.\n",
                "\n",
                "### Dilated Windows\n",
                "\n",
                "Longformer also supports dilated sliding windows where gaps are introduced:\n",
                "\n",
                "$$\\text{Attend}_{\\text{dilated}}(i) = \\{j : |i - j| \\leq w \\cdot d, (i-j) \\mod d = 0\\}$$\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- Time: $O(n \\cdot w \\cdot d)$\n",
                "- Space: $O(n \\cdot w)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sliding_window_mask(seq_len: int, window_size: int):\n",
                "    \"\"\"Create sliding window attention mask.\"\"\"\n",
                "    positions = torch.arange(seq_len)\n",
                "    diff = positions.unsqueeze(0) - positions.unsqueeze(1)\n",
                "    mask = torch.abs(diff) > window_size\n",
                "    return mask\n",
                "\n",
                "class SlidingWindowAttention(nn.Module):\n",
                "    \"\"\"Sliding Window Attention for local context modeling.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model: int, num_heads: int = 8, window_size: int = 256) -> None:\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        self.d_model = d_model\n",
                "        self.window_size = window_size\n",
                "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
                "        \n",
                "        self.w_q = nn.Linear(d_model, d_model)\n",
                "        self.w_k = nn.Linear(d_model, d_model)\n",
                "        self.w_v = nn.Linear(d_model, d_model)\n",
                "        self.w_o = nn.Linear(d_model, d_model)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor):\n",
                "        batch, seq_len = x.size(0), x.size(1)\n",
                "        \n",
                "        q = self.w_q(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        k = self.w_k(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        v = self.w_v(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
                "        \n",
                "        # Apply sliding window mask\n",
                "        window_mask = create_sliding_window_mask(seq_len, self.window_size).to(x.device)\n",
                "        scores = scores.masked_fill(window_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
                "        \n",
                "        attn = F.softmax(scores, dim=-1)\n",
                "        attn = torch.nan_to_num(attn, nan=0.0)\n",
                "        \n",
                "        attended = torch.matmul(attn, v)\n",
                "        attended = attended.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)\n",
                "        \n",
                "        return self.w_o(attended), attn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize sliding window patterns\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "seq_len = 16\n",
                "\n",
                "for idx, window in enumerate([2, 4, 8]):\n",
                "    mask = create_sliding_window_mask(seq_len, window)\n",
                "    pattern = (~mask).float().numpy()\n",
                "    \n",
                "    sns.heatmap(pattern, ax=axes[idx], cmap='Blues', square=True, cbar=False)\n",
                "    axes[idx].set_title(f'Window Size = {window}')\n",
                "    axes[idx].set_xlabel('Key Position')\n",
                "    axes[idx].set_ylabel('Query Position')\n",
                "\n",
                "plt.suptitle('Sliding Window Attention Patterns')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Complexity comparison\n",
                "seq_lengths = [512, 1024, 2048, 4096, 8192]\n",
                "window_size = 256\n",
                "d = 64\n",
                "\n",
                "print(f\"Complexity comparison (window={window_size}):\")\n",
                "print(\"-\" * 60)\n",
                "print(f\"{'Seq Len':<10} {'Full O(n^2*d)':<20} {'Window O(n*w*d)':<20}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for n in seq_lengths:\n",
                "    full = n * n * d\n",
                "    windowed = n * window_size * d\n",
                "    print(f\"{n:<10} {full:<20,} {windowed:<20,} ({full/windowed:.1f}x)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## When to Use Sliding Window\n",
                "\n",
                "| Task | Sliding Window? |\n",
                "|------|----------------|\n",
                "| Long documents | Yes |\n",
                "| Local patterns (NER, POS) | Yes |\n",
                "| Global reasoning needed | Add global tokens |\n",
                "| Short sequences (<512) | Use full attention |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}