{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Grouped-Query Attention (GQA)\n",
                "\n",
                "## Paper Reference\n",
                "\n",
                "**Ainslie, J., et al. (2023).** *GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.* [arXiv:2305.13245](https://arxiv.org/abs/2305.13245)\n",
                "\n",
                "---\n",
                "\n",
                "## Key Insight\n",
                "\n",
                "GQA interpolates between MHA and MQA by grouping query heads:\n",
                "\n",
                "- **MHA**: $G = H$ (each head has its own K/V)\n",
                "- **GQA**: $1 < G < H$ (heads grouped, share K/V within group)\n",
                "- **MQA**: $G = 1$ (all heads share single K/V)\n",
                "\n",
                "### Formula\n",
                "\n",
                "If $H$ query heads and $G$ KV heads:\n",
                "- Each group has $H/G$ query heads\n",
                "- Query heads in same group share K/V\n",
                "\n",
                "### Used In\n",
                "\n",
                "- LLaMA 2/3\n",
                "- Mistral\n",
                "- Many modern LLMs\n",
                "\n",
                "### Complexity\n",
                "\n",
                "- KV Cache: $O(n \\cdot d_k \\cdot G)$ - scales with number of KV groups"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GroupedQueryAttention(nn.Module):\n",
                "    \"\"\"Grouped-Query Attention: groups of Q heads share K/V.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model: int, num_heads: int = 8, num_kv_heads: int = 4) -> None:\n",
                "        super().__init__()\n",
                "        assert d_model % num_heads == 0\n",
                "        assert num_heads % num_kv_heads == 0\n",
                "        \n",
                "        self.num_heads = num_heads\n",
                "        self.num_kv_heads = num_kv_heads\n",
                "        self.num_groups = num_heads // num_kv_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        self.d_model = d_model\n",
                "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
                "        \n",
                "        self.w_q = nn.Linear(d_model, d_model)\n",
                "        self.w_k = nn.Linear(d_model, num_kv_heads * self.d_k)\n",
                "        self.w_v = nn.Linear(d_model, num_kv_heads * self.d_k)\n",
                "        self.w_o = nn.Linear(d_model, d_model)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor):\n",
                "        batch, seq_len = x.size(0), x.size(1)\n",
                "        \n",
                "        q = self.w_q(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
                "        k = self.w_k(x).view(batch, seq_len, self.num_kv_heads, self.d_k).transpose(1, 2)\n",
                "        v = self.w_v(x).view(batch, seq_len, self.num_kv_heads, self.d_k).transpose(1, 2)\n",
                "        \n",
                "        # Repeat K/V for each group\n",
                "        k = k.repeat_interleave(self.num_groups, dim=1)\n",
                "        v = v.repeat_interleave(self.num_groups, dim=1)\n",
                "        \n",
                "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
                "        attn = F.softmax(scores, dim=-1)\n",
                "        \n",
                "        attended = torch.matmul(attn, v)\n",
                "        attended = attended.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)\n",
                "        \n",
                "        return self.w_o(attended), attn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize grouping structure\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "configs = [\n",
                "    ('MHA (G=8)', 8, 8),\n",
                "    ('GQA (G=4)', 8, 4),\n",
                "    ('MQA (G=1)', 8, 1),\n",
                "]\n",
                "\n",
                "for idx, (name, num_heads, num_kv_heads) in enumerate(configs):\n",
                "    ax = axes[idx]\n",
                "    \n",
                "    # Create visualization\n",
                "    groups = num_heads // num_kv_heads\n",
                "    colors = plt.cm.tab10.colors\n",
                "    \n",
                "    # Q heads\n",
                "    for h in range(num_heads):\n",
                "        group = h // groups\n",
                "        ax.barh(h, 1, color=colors[group % len(colors)], edgecolor='black')\n",
                "    \n",
                "    ax.set_yticks(range(num_heads))\n",
                "    ax.set_yticklabels([f'Q{i}' for i in range(num_heads)])\n",
                "    ax.set_xlabel(f'KV Group (total: {num_kv_heads})')\n",
                "    ax.set_title(name)\n",
                "    ax.invert_yaxis()\n",
                "\n",
                "plt.suptitle('Query Head to KV Head Grouping')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# KV cache comparison\n",
                "d_model, num_heads, seq_len = 4096, 32, 2048\n",
                "d_k = d_model // num_heads\n",
                "\n",
                "configs = [\n",
                "    ('MHA', num_heads),\n",
                "    ('GQA-8', 8),\n",
                "    ('GQA-4', 4),\n",
                "    ('GQA-2', 2),\n",
                "    ('MQA', 1),\n",
                "]\n",
                "\n",
                "print(\"KV Cache Size Comparison (per layer, fp16):\")\n",
                "print(\"-\" * 50)\n",
                "for name, kv_heads in configs:\n",
                "    cache_size = seq_len * d_k * kv_heads * 2 * 2  # K+V, 2 bytes\n",
                "    print(f\"{name}: {cache_size / 1024**2:.2f} MB  ({kv_heads} KV heads)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## GQA in Production Models\n",
                "\n",
                "| Model | Q Heads | KV Heads | Ratio |\n",
                "|-------|---------|----------|-------|\n",
                "| LLaMA 2 7B | 32 | 32 | MHA |\n",
                "| LLaMA 2 70B | 64 | 8 | GQA-8 |\n",
                "| Mistral 7B | 32 | 8 | GQA-4 |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}